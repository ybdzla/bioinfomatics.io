# 8.1)
```r
# 1. 加载所需 R 包
install.packages("glmnet")
library(caret)
library(pROC)
library(glmnet)
library(ggplot2)


# 2. 数据读取与预处理
# 读取数据
data <- read.csv("qPCR_data.csv")

# 提取特征和标签
x <- data[, 2:12]
y <- as.factor(data[, 13])  # 标签列：NC / HCC

# 清洗列名（防止非法字符）
colnames(x) <- make.names(colnames(x))

# 转为数值型 + 缺失值填补（均值）
x <- apply(x, 2, as.numeric)
feature.mean <- colMeans(x, na.rm = TRUE)
x[is.na(x)] <- matrix(rep(feature.mean, each = nrow(x)), nrow = nrow(x))[is.na(x)]

# Z-score 标准化
x <- scale(x, center = TRUE, scale = TRUE)

# 3.PCA可视化
pca.res <- prcomp(x, center = TRUE, scale. = TRUE)
pca.df <- data.frame(pca.res$x[, 1:2], label = y)

ggplot(pca.df, aes(x = PC1, y = PC2, color = label)) +
  geom_point(size = 3) +
  labs(title = "PCA of qPCR data", x = "PC1", y = "PC2") +
  theme_minimal()

# 4.数据集划分（80%训练集，20%测试集）
set.seed(666)
train.idx <- createDataPartition(y, p = 0.8, list = FALSE)
x.train <- x[train.idx, ]
x.test  <- x[-train.idx, ]
y.train <- y[train.idx]
y.test  <- y[-train.idx]

# 5.特征选用（使用RFE）
lrFuncs$summary <- twoClassSummary
ctrl <- rfeControl(functions = lrFuncs, method = "boot", number = 10, verbose = TRUE)

rfe.res <- rfe(x.train, y.train, sizes = 2:11, rfeControl = ctrl, metric = "ROC")
selected.vars <- predictors(rfe.res)
selected.vars  # 输出被选中的变量
#[1] "SNORD3B" "miR.122" "miR.223"

# 6.模型训练 + 超参数调优（逻辑回归）
params.grid <- expand.grid(alpha = c(0, 0.5, 1), lambda = c(0.01, 0.1, 1))

tr.ctrl <- trainControl(method = "cv", number = 5,
                        summaryFunction = twoClassSummary,
                        classProbs = TRUE)

cv.fit <- train(x.train[, selected.vars], y.train,
                method = "glmnet",
                family = "binomial",
                metric = "ROC",
                tuneGrid = params.grid,
                trControl = tr.ctrl)

cv.fit$bestTune  # 查看最优参数
## 输出结果
  alpha lambda
4   0.5   0.01

# 7.模型评估
y.prob <- predict(cv.fit, newdata = x.test[, selected.vars], type = "prob")[, "HCC"]
roc.curve <- roc(y.test, y.prob, levels = rev(levels(y.test)))

plot(roc.curve, print.auc = TRUE, main = "ROC Curve for qPCR Classification")
```
## 图1：PCA降维
![image](https://github.com/user-attachments/assets/d24f529b-b25d-4c4d-9ad7-d8c43986974b)
## 图2：ROC曲线
![image](https://github.com/user-attachments/assets/32dfc65b-b21b-4696-8433-a9fd6396a47b)

---

# 8.2)
## 问题一：树的数量是不是一个需要通过交叉验证调整的超参数？为什么？

**答:**  
树的数量（`ntree`）虽然是随机森林中的一个超参数，但**通常不需要通过交叉验证精细调整**。

**原因：**

- 随机森林是一种**集成方法**，通过多棵树的投票来提升稳定性；
- 当树的数量足够大时，模型的**泛化误差趋于稳定**，进一步增加树的数量不会明显改变模型性能；
- 在实际应用中，`ntree` 设置得足够大（如 500 或 1000）即可，**不会导致过拟合**；
- 增加树的数量主要影响的是**训练时间**和**计算资源**，而不是模型表现。

---

## 问题二：什么是随机森林的 OOB（Out-of-Bag）误差？它与 bootstrapping 有什么关系？

**定义：**

- **Out-of-Bag (OOB) error** 是一种在训练过程中自动得到的**模型误差估计**；
- 它利用**每棵树未被选中用于训练的样本**来评估该树的预测能力，无需单独划分验证集。

**与 Bootstrapping 的关系：**

- 随机森林训练每棵树时使用的是**自助采样法（Bootstrapping）**：从原始训练集中有放回地抽样；
- 每棵树约使用了原始样本的 **63.2%**（理论值为 $1 - \frac{1}{e} \approx 0.632$）；
- 剩下的约 **36.8%** 样本没有被该树使用，称为该树的 **OOB 样本**；
- 用这些 OOB 样本在对应树上预测，计算错误率，**所有树的 OOB 误差平均值就是最终的 OOB error**。

**优点：**

- 不需要额外的验证集或交叉验证；
- 估计误差效率高、稳定，**尤其适用于样本量不大**的任务；
- 在 R 的 `randomForest` 包中默认输出 OOB error。



